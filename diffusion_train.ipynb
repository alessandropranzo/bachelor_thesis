{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Problem\n",
    "## Diffusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import DiffusionNN, make_forward_fn, make_diffusion_loss\n",
    "import torch\n",
    "import torchopt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants to initialize\n",
    "x_domain = (-1, 1)\n",
    "t_domain = (0, 1)\n",
    "learning_rate = 0.01\n",
    "n_epochs = 10000\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model and functional setup\n",
    "diffusion_model = DiffusionNN()\n",
    "diffusion_function = make_forward_fn(diffusion_model)\n",
    "\n",
    "diffusion_loss = make_diffusion_loss(diffusion_function)\n",
    "\n",
    "optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=learning_rate))\n",
    "\n",
    "params = tuple(diffusion_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Training the network with a random uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the training process\n",
    "loss_evolution = []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training process:\", unit=\"epoch\"):\n",
    "    #Let's start with a uniform distribution of the data \n",
    "    x = torch.FloatTensor(batch_size).uniform_(x_domain[0], x_domain[1])\n",
    "    t = torch.FloatTensor(batch_size).uniform_(t_domain[0], t_domain[1])\n",
    "\n",
    "    #We compute the loss\n",
    "    loss = diffusion_loss(x, t, params)\n",
    "    #Update the parameters with the functional optimizer\n",
    "    params = optimizer.step(loss, params)\n",
    "    #Keeping track of the loss\n",
    "    loss_evolution.append(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(loss_evolution) + 1), loss_evolution, linestyle='-')\n",
    "plt.title('Loss Evolution Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants to initialize\n",
    "x_domain = (-1, 1)\n",
    "t_domain = (0, 1)\n",
    "learning_rate = 0.01\n",
    "n_epochs = 10000\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model and functional setup\n",
    "diffusion_model_2 = DiffusionNN([2, 10, 10, 10, 10, 1])\n",
    "diffusion_function_2 = make_forward_fn(diffusion_model_2)\n",
    "\n",
    "diffusion_loss_2 = make_diffusion_loss(diffusion_function_2)\n",
    "\n",
    "optimizer_2 = torchopt.FuncOptimizer(torchopt.adam(lr=learning_rate))\n",
    "\n",
    "params_2 = tuple(diffusion_model_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the training process\n",
    "loss_evolution_2 = []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training process:\", unit=\"epoch\"):\n",
    "    #Let's start with a uniform distribution of the data \n",
    "    x = torch.FloatTensor(batch_size).uniform_(x_domain[0], x_domain[1])\n",
    "    t = torch.FloatTensor(batch_size).uniform_(t_domain[0], t_domain[1])\n",
    "\n",
    "    #We compute the loss\n",
    "    loss = diffusion_loss_2(x, t, params_2)\n",
    "    #Update the parameters with the functional optimizer\n",
    "    params_2 = optimizer_2.step(loss, params_2)\n",
    "    #Keeping track of the loss\n",
    "    loss_evolution_2.append(float(loss))\n",
    "\n",
    "torch.save(diffusion_model_2.state_dict(), \"model_parameters.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second batch\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training process:\", unit=\"epoch\"):\n",
    "    #Let's start with a uniform distribution of the data \n",
    "    x = torch.FloatTensor(batch_size).uniform_(x_domain[0], x_domain[1])\n",
    "    t = torch.FloatTensor(batch_size).uniform_(t_domain[0], t_domain[1])\n",
    "\n",
    "    #We compute the loss\n",
    "    loss = diffusion_loss_2(x, t, params_2)\n",
    "    #Update the parameters with the functional optimizer\n",
    "    params_2 = optimizer_2.step(loss, params_2)\n",
    "    #Keeping track of the loss\n",
    "    loss_evolution_2.append(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(loss_evolution_2) + 1), loss_evolution_2, linestyle='-')\n",
    "plt.title('Loss Evolution Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
